---
title: "Significance Bands for Local Projetsions in R"
author: "Itamar Caspi"
date: "`r Sys.Date()`"
output:
  html_document:
    code_folding: show
    highlight: haddock
    keep_md: no
    theme: journal
    toc: yes
    toc_depth: 4
    toc_float: yes
abstract: This notebook replicates the main finding presented in the paper "Significance Bands for Local Projections" by [Inoue, Jordà, and Kuersteiner (2023)](https://arxiv.org/abs/2306.03073). The original paper outlines the importance of using significance bands instead of traditional confidence bands for evaluating the dynamic impact of a treatment on an outcome variable via local projections. In particular, the note employs the R programming language to implement these methods, providing a detailed, step-by-step guide for researchers interested in applying them in their own analyses.
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(
  eval    = TRUE,
  echo    = TRUE,
  warning = FALSE,
  message = FALSE
)
```

## Introduction

[Inoue, Jordà, and Kuersteiner (2023)](https://arxiv.org/abs/2306.03073) propose employing significance bands with confidence bands to enhance impulse response function visualization. This approach allows for a more nuanced interpretation of statistical estimates. Confidence bands provide information on the uncertainty of each impulse response coefficient. In contrast, significance bands are better suited for visually assessing the null hypothesis, which posits that the entire impulse response is zero.

Standard statistical software and the Lagrange multiplier principle make constructing significance bands straightforward under the null hypothesis. This ease of construction is a noteworthy advantage. The bands' calculations do not depend on the impulse response horizon, and the paper provides approximate formulas. The authors also discuss bootstrap methods as a less assumption-reliant approach.

Monte Carlo simulations show that the proposed significance bands outperform traditional confidence bands. The simulations reveal that these bands have favorable size and power properties compared to solely considering whether confidence bands include zero. Thus, the authors argue that significance bands should be a standard graphical representation for impulse response functions.

In this paper, I replicate key findings from Inoue, Jordà, and Kuersteiner's study. Specifically, I focus on estimating local projections (LPs) for the Romer and Romer monetary policy shock. I then test these estimates using the significance bands method.

## Load packages

Start by loading the necessary packages

```{r}
library(tidyverse) # for data processing and plotting
library(broom)     # for tidying regression output
library(readxl)    # for reading .xls files
library(dynlm)     # for running dynamic regressions with leads and lags
library(sandwich)  # for Newey-West standard errors
library(lmtest)    # for coefficient tests
library(car)       # for linear hypothesis tests
```

## Basic setup

Set the basic parameters

```{r}
H     <- 17    # IRF horizon
pval  <- 0.05  # p-value
lags  <- 6     # lags to be used in LP regressions
nwlag <- H     # lags to be used in Newey-West
block <- H     # bootstrap block size
```

Set the sample

```{r}
smpl_start <- ymd("1985-01-01")
smpl_end   <- ymd("2007-12-31")
```

## Data

Load the `.dta` file

```{r}
df_raw <- read_xls("data/data.xls")
```

Some preprocessing steps include scaling the response variable lcpi to work in percent, renaming the response variable to y, and renaming the shock variable rr_shock to z.

```{r}
df <- df_raw %>% 
  mutate(
    date   = ymd(DATE),
    lcpi   = 100 * log(GDPDEF),
    dlcpi  = c(NA, 4 * diff(lcpi)),
    lrgdp  = 100 * 4 * log(GDPC1),
    dlrgdp = c(NA, diff(lrgdp)),
    dstir  = c(NA, diff(FEDFUNDS))
  ) %>% 
  rename(
    y = lcpi,      # response variable
    z = RRSHOCK    # Romer and Romer shock
  ) %>% 
  filter(between(date, smpl_start, smpl_end)) %>% 
  select(date, y, z, dlcpi, dlrgdp, dstir)
```

Add lthe ong differences of $y_t$, i.e., $\Delta^h y_{t+h} \equiv y_{t+h} - y_{t-1}$ for $h=1,…,H$

```{r}
df_fwd <- df %>% 
  bind_cols(
    map_dfc(0:17, ~ df %>%
             transmute(!!paste0("ldy", .x) := lead(y, .x) - lag(y, 1)))
  )
```

## Orthogonalization

Define the portion of the regression formula that includes both the constant and the control variables:

```{r}
formula_x = "1 + L(dlrgdp, 1:lags) + L(dlcpi, 1:lags) + L(dstir, 1:lags)"
```

Orthogonalize $z_t$ with respect to the controls and store the residual in a variable named `r_z`:

```{r}
formula_z <- paste0("z ~ ", formula_x) %>% as.formula()

model_z <- dynlm(formula_z, data = df_fwd %>% ts())

df_r_z <- model_z$residuals %>% 
  as_tibble() %>% 
  rename(r_z = 1)
```

Orthogonalize $\Delta_h y_{t+h}$ for $h=0,...,H$ with respect to the controls and store the residuals in `r_fy0`,...,`r_fyH`:

```{r}
mat_r_y <- matrix(NA, length(df_r_z$r_z), H+1)

for (i in 0:H) {
  
  formula_y <- paste0("ldy", i, "~ ", formula_x) %>%  as.formula()
  model_y   <- dynlm(formula_y, data = df_fwd %>% ts())
  
  if (i == 0) {
    mat_r_y[ , i+1] <- model_y$residuals %>% as.vector()
  } else{
    mat_r_y[ , i+1] <- c(model_y$residuals %>% as.vector(), rep(NA, i))
  }

}

df_r_y <- mat_r_y %>% 
  as_tibble()

colnames(df_r_y) <- paste0("r_fy", 0:H)
```

Combine the orthogonalized residuals into a single dataframe:

```{r}
df_r_yz <- df_r_y %>% 
  bind_cols(df_r_z) %>% 
  mutate(date = 1:n()) 
```

## Local projections

Run the local projection procedure using the orthogonalized series. Specifically, the code below estimates the given regression model:

$$
ry_{t+h}=rz_t \beta_h+u_{t+h} \quad \text { for } h=0,1, \ldots, H-1 ; \quad t=1, \ldots, T
$$

```{r}
mat_lp <- matrix(data = NA, ncol = 7, nrow = H+1)  # matrix to store estimation results

for (i in 0:H) {
  
  formula_r_y <- paste0("r_fy", i, " ~ r_z -1") %>% as.formula()
  lp_fit      <- dynlm(formula_r_y, data = df_r_yz %>% ts())
  
  nw_se     <- sqrt(diag(NeweyWest(lp_fit, lag = i)))
  n         <- length(lp_fit$residuals)
  nw_se_adj <- nw_se * sqrt((n - 1) / (n - 20))

  beta_i      <- lp_fit$coefficients["r_z"]
  se_beta_i   <-  nw_se_adj["r_z"]
  
  mat_lp[i+1, 1] <- i
  mat_lp[i+1, 2] <- beta_i
  mat_lp[i+1, 3] <- se_beta_i
  mat_lp[i+1, 4] <- beta_i + 1 * se_beta_i
  mat_lp[i+1, 5] <- beta_i - 1 * se_beta_i
  mat_lp[i+1, 6] <- beta_i + 2 * se_beta_i
  mat_lp[i+1, 7] <- beta_i - 2 * se_beta_i

}

df_lp <- mat_lp %>% 
  as_tibble()

colnames(df_lp) <- 
  c("horizon", "mid", "se", "up1sd", "down1sd", "up2sd", "down2sd")
```

Plot the LP and confidence bands (one and two standard deviations):

```{r}
df_lp %>% 
  ggplot(aes(x = horizon)) +
  geom_line(aes(y = mid), color = 'darkgreen', size = 1.5) +
  geom_ribbon(aes(ymin = down1sd, ymax = up1sd),
              fill = "darkgreen", linetype = 0, alpha = 0.2) +
  geom_ribbon(aes(ymin = down2sd, ymax = up2sd),
              fill = "darkgreen", linetype = 0, alpha = 0.1) +
  scale_x_continuous(expand = c(0,0), breaks = seq(0, H, 5)) +
  scale_y_continuous(expand = c(0,0), limits = c(-6, 2)) +
  labs(
    x = "Quearter",
    y = "Percent",
    title = "Cumulative response of the CPI in percent to a Romer monetary shock",
  ) +
  geom_hline(aes(yintercept = 0)) +
  theme_light(12, base_family = "Palatino Linotype") +
  theme(
    panel.grid.major = element_blank(),
    panel.grid.minor = element_blank()
  )
```

The figure illustrates the impulse response of 100 times the logarithm of the GDP deflator following a Romer and Romer (2004) monetary olicy shock (extended by Wieland and Yang, 2020.) The model includes four lagged variables for the GDP deflator, real GDP growth, and the Federal Funds Rate. Dark and light green shaded areas indicate bands corresponding to one and two standard errors, respectively. For further details, refer to [Inoue, Jordà, and Kuersteiner (2023.)](#0)

## Significance bands

#### Significance bands using asymptotic approximation:

1.  Calculate the sample average of the product (the orthogonalized) $s_t z_t$. Call this $\hat{\gamma}_{s z}$.
2.  Construct the auxiliary variable $\eta_t=y_t z_t$ and regress $\eta_t$ on a constant. The Newey-West estimate of the standard error of the intercept coefficient is an estimate of $s_{\hat{\eta}}$.
3.  An estimate of $\sigma / \sqrt{T-h}$, call it $\hat{s}_{\beta_h}$, is therefore: $$
    \hat{s}_{\beta_h}=\frac{\hat{s}_\eta}{\hat{\gamma}_{s z}}
    $$
4.  Construct the significance bands as: $$
    \left[\zeta_{\alpha / 2 H} \hat{\mathrm{s}}_{\beta_h}, \zeta_{1-\alpha / 2 H} \hat{\mathrm{s}}_{\beta_h}\right]
    $$

```{r}
gamma      <- df_r_yz$r_z^2
gamma_mean <- mean(gamma)

eta        <- df_r_yz$r_fy0 * df_r_yz$r_z^2
model_eta  <- lm(eta ~ 1)
nw_se_eta  <- sqrt(diag(NeweyWest(model_eta, lag = nwlag)))
seta       <- nw_se_eta["(Intercept)"]

sbeta      <- seta/gamma_mean

bju <- qnorm(1 - (pval / (2 * (H + 1)))) * sbeta
bjd <- -bju
```

#### Significance bands using the Wold-Block Bootstrap:

1.  Calculate the sample average of $s_t z_t$. Call this $\hat{\gamma}_{s z}$.
2.  Construct the auxiliary variable $\eta_t=y_t z_t$ and regress $\eta_t$ on a constant. The Wild Block bootstrap estimate of the standard error of the intercept coefficient is an estimate of $s_{\hat{\eta}}$.
3.  An estimate of $\sigma / \sqrt{T-h}$, call it $\hat{s}_{\beta_h}^b$, is therefore: $$
    \hat{s}_{\beta_h}^b=\frac{\hat{s}_{\eta \eta}^b}{\hat{\gamma}_{s z}}
    $$
4.  Construct the significance bands as: $$
    \left[\zeta_{\alpha / 2 H} \hat{s}_{\beta_{h^{\prime}}}^b \zeta_{1-\alpha / 2 H} \hat{s}_{\beta_h}^b\right]
    $$

```{r}
set.seed(12345)   # for replication
n         <- length(df_r_yz$r_fy0)
block_id  <- floor((1:n + block - 1) / block)

model_eta <- lm(eta ~ 1)
nw_se_eta <- sqrt(diag(vcovBS(model_eta, cluster = block_id, R = 1000)))
seta      <- nw_se_eta["(Intercept)"]

sbeta     <- seta/gamma_mean

bootju <- qnorm(1 - (pval / (2 * (H + 1)))) * sbeta
bootjd <- -bootju
```

Combine the estimation results of significance bands into a single dataframe:

```{r}
df_sb <- tibble(
  bju    = rep(bju, H+1),
  bjd    = rep(bjd, H+1),
  bootju = rep(bootju, H+1),
  bootjd = rep(bootjd, H+1)
)
```

The following figure adds classical NW significance bands, shown in dashed blue and bootstrap significance bands, shoen in dashed red.

```{r}
df_lp %>% 
  bind_cols(df_sb) %>% 
  ggplot(aes(x = horizon)) +
  geom_line(aes(y = mid), color = 'darkgreen', size = 1.5) +
  geom_line(aes(y = bju), color = "red", linetype = 2, size = 1) +
  geom_line(aes(y = bjd), color = "red", linetype = 2, size = 1) +
  geom_line(aes(y = bootju), color = "blue", linetype = 2, size = 1) +
  geom_line(aes(y = bootjd), color = "blue", linetype = 2, size = 1) +
  geom_ribbon(aes(ymin = down1sd, ymax = up1sd),
              fill = "darkgreen", linetype = 0, alpha = 0.2) +
  geom_ribbon(aes(ymin = down2sd, ymax = up2sd),
              fill = "darkgreen", linetype = 0, alpha = 0.1) +
  scale_x_continuous(expand = c(0,0), breaks = seq(0, H, 5)) +
  scale_y_continuous(expand = c(0,0), limits = c(-6, 2)) +
  labs(
    x = "Quearter",
    y = "Percent",
    title = "Cumulative response of the CPI in percent to a Romer monetary shock",
  ) +
  geom_hline(aes(yintercept = 0)) +
  theme_light(12, base_family = "Palatino Linotype") +
  theme(
    panel.grid.major = element_blank(),
    panel.grid.minor = element_blank()
  )
```

## Stacked LPs

In this section, we employ a technique proposed by Jordà, where we estimate the $h$ local projection regression simultaneously as a panel anduse Driscoll-Kraay HAC robust standard errors for inference. The advantage of this estimation method is that it provides us with the coefficients' variance-covariance matrix, thereby enabling us to test the hypothesis that $\beta_h=0$ for all $h$.

We start by pivoting the data to long format

```{r}
df_r_yz_long <- df_r_yz %>%
  pivot_longer(-c(date, r_z), names_to = "horizon", values_to = "r_fy") %>%
  mutate(
    horizon = as.factor(gsub("^r_fy", "", horizon))
  ) %>% 
  select(date, horizon, r_fy, r_z)
```

Next, we estimate the data stacked as a panel using the `plm` package:

```{r}
library(plm)  # to estimate panel data models

plm_model <- plm(r_fy ~ factor(horizon):r_z, data = df_r_yz_long, index = c("horizon", "date"), model = "within")
```

Finally, we conduct the joint hypothesis test, namely: $$
H_0: \beta_h=0, \text{for } h=0,\ldots,H
$$

```{r}
hypothesis <- sapply(0:H, function(i) paste0("factor(horizon)", i, ":r_z = 0"))

linearHypothesis(plm_model, hypothesis, vcov. = function(x) vcovSCC(x, maxlag=nwlag))
```
